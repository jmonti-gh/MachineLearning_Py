{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle - Learn: Intro to Machine Learning\n",
    "- https://www.kaggle.com/learn/intro-to-machine-learning\n",
    "## 5. Underfitting and Overfitting\n",
    "- Fine-tune your model for better performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting With Different Models\n",
    "- Decision Tree Model (DecisionTreeRegressor) has many options, most important options determine the tree's depth\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "    - https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "- A shallow tree can generate an __underfitting__ model.\n",
    "- A tree that is too deep can lead to __overfitting__.\n",
    "- __Underfitting:__ failing to capture relevant patterns, again leading to less accurate predictions.\n",
    "- __Overfitting:__ capturing spurious patterns that won't recur in the future, leading to less accurate predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example \n",
    "*max_leaf_nodes* argument provides a very sensible way to control overfitting vs underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  322937\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  303430\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  302932\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  303443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "\n",
    "# Load data\n",
    "melbourne_df = pd.read_csv('min_melb_data.csv') \n",
    "# Filter rows with missing values\n",
    "filtered_melbourne_df = melbourne_df.dropna(axis=0)\n",
    "# Choose target and features\n",
    "y = filtered_melbourne_df.Price\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n",
    "                        'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = filtered_melbourne_df[melbourne_features]\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n",
    "\n",
    "\n",
    "# compare MAE with differing values of max_leaf_nodes\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimal max_leaf_nodes = 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Underfitting and Overfitting\n",
    "- Optimize the size of the tree to make better predictions. In next cell the known code for the model previously built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- MAE: 29,652.9315 ----\n",
      "MAE % of real mean: 16.78 %  - real mean (val_y.mean()): 176,725.51\n"
     ]
    }
   ],
   "source": [
    "# --> Better to use split data (train + val -or test- data)\n",
    "### Build a model with train data and validate (quality measure) with validation data\n",
    "\n",
    "# 0.- import libraries, modules, functions i'll need.\n",
    "import pandas as pd                                 # to get & manage df and Series\n",
    "from sklearn.tree import DecisionTreeRegressor      # to define e model type\n",
    "from sklearn.model_selection import train_test_split    # to split train & validation (test) data from whole df\n",
    "from sklearn.metrics import mean_absolute_error     # to calc. MAE\n",
    "\n",
    "# 1.- read data + basic filter missing values +  obtain target & features + ...\n",
    "df = pd.read_csv('train.csv')\n",
    "#df = df_0.dropna(axis=0)  ---- OJO __ analizar esto para distintos datasets!!!\n",
    "y = df.SalePrice\n",
    "X = df[['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']]\n",
    "        # (df.column if i want to know what are the columns)\n",
    "# i won't make de non-split-data model - ONLY the more real model de splitted one\n",
    "\n",
    "# 2. - split data (between train and validation) + make and fit this model\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "df_model = DecisionTreeRegressor(random_state=1)\n",
    "df_model.fit(train_X, train_y)\n",
    "\n",
    "# 3.- Measure model quality: calc predict, calc mae, (optional) calc % mae / average_target\n",
    "val_predict = df_model.predict(val_X)\n",
    "mae = mean_absolute_error(val_y, val_predict)\n",
    "print(f' MAE: {round(mae, 4):,} '.center(26, '-'))\n",
    "ratio_mae_avg = mae / val_y.mean()\n",
    "print(f'MAE % of real mean: {round(ratio_mae_avg * 100, 2)} %\\\n",
    "  - real mean (val_y.mean()): {round(val_y.mean(), 2):,}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same construction with kaggle terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 29,652.93\n"
     ]
    }
   ],
   "source": [
    "### Code you have previously used to load data\n",
    "# imports...\n",
    "\n",
    "home_data = pd.read_csv('train.csv')\n",
    "# Create target object and call it y\n",
    "y = home_data.SalePrice\n",
    "# Create X\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = home_data[features]\n",
    "\n",
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Specify Model\n",
    "iowa_model = DecisionTreeRegressor(random_state=1)\n",
    "# Fit Model\n",
    "iowa_model.fit(train_X, train_y)\n",
    "\n",
    "# Make validation predictions and calculate mean absolute error\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_predictions, val_y)\n",
    "print(\"Validation MAE: {:,.2f}\".format(val_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get_mae() is just written..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compare Different Tree Sizes\n",
    "Write a loop that tries the following values for max_leaf_nodes from a set of possible values.\n",
    "\n",
    "Call the get_mae function on each value of max_leaf_nodes. Store the output in some way that allows you to select the value of max_leaf_nodes that gives the most accurate model on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_leaf_nodes: 5 \t\t MAE: 35,044.51\n",
      "max_leaf_nodes: 25 \t\t MAE: 29,016.41\n",
      "max_leaf_nodes: 50 \t\t MAE: 27,405.93\n",
      "max_leaf_nodes: 100 \t\t MAE: 27,282.51\n",
      "max_leaf_nodes: 250 \t\t MAE: 27,893.82\n",
      "max_leaf_nodes: 500 \t\t MAE: 29,454.19\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "for max_lnodes in candidate_max_leaf_nodes:\n",
    "    my_mae = get_mae(max_lnodes, train_X, val_X, train_y, val_y)\n",
    "    print(f'max_leaf_nodes: {max_lnodes} \\t\\t MAE: {my_mae:,.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> beast_tree_size = 100 (best max_leaf_nodes value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit Model Using All Data\n",
    "- You know the best tree size. If you were going to deploy this model in practice, you would make it even more accurate by using all of the data and keeping that tree size. That is, you don't need to hold out the validation data now that you've made all your modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_leaf_nodes=100, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_leaf_nodes=100, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=100, random_state=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in argument to make optimal size and uncomment\n",
    "final_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=0)\n",
    "\n",
    "# fit the final model and uncomment the next two lines\n",
    "final_model.fit(X, y)   # ALL the Data used give more accurate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You've tuned this model and improved your results. But we are still using Decision Tree models, which are not very sophisticated by modern machine learning standards. In the next step you will learn to use Random Forests to improve your models even more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
