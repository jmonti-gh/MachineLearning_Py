{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle - Learn: Intermediate Machine Learning\n",
    "- https://www.kaggle.com/learn/intermediate-machine-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Missing Values\n",
    "- Missing values happen. Be prepared for this common challenge in real datasets.\n",
    "- Most machine learning libraries (including scikit-learn) give an error if you try to build a model using data with missing values.\n",
    "-  you will learn three approaches to __dealing with missing values__. Then you'll compare the effectiveness of these approaches on a real-world dataset.\n",
    "    1. A Simple Option: Drop Columns with Missing Values\n",
    "    2. A Better Option: Imputation - fill missing value with some number (ex. mean())\n",
    "    3. An Extension To Imputation - 2. + new bool missing column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Case\n",
    "- In the example, we will work with min_melb_data.csv dataset in files/ subdir. Our model will use information such as the number of rooms and land size to predict home price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2679, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('files/min_melb_data.csv')     # Load the data\n",
    "print(df.shape)     #display(df)\n",
    "y = df.Price                                    # Select target\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors (features). NEW!\n",
    "melb_predictors = df.drop(['Price'], axis=1)            # new df w/o Price col\n",
    "X = melb_predictors.select_dtypes(exclude=['object'])   # new df with only numerical cols\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0,\n",
    "                                                        train_size=0.8,\n",
    "                                                        test_size=0.2)\n",
    "\n",
    "### JM TOUCH cause ALL the cols have missing values, look\n",
    "## df.info() mmm¿?\n",
    "#X_valid.info()\n",
    "#X_train.info()\n",
    "#display(X_train[pd.isnull(X_train.Distance)])  # row[1920]\n",
    "X_train.drop([1920], inplace=True)\n",
    "#display(X_train[pd.isnull(X_train.Distance)])  # row[1920]\n",
    "#X_train.info()\n",
    "## Now we have not all cols with NaN.. --> in X_train!!\n",
    "#display(X_valid[pd.isnull(X_valid.Distance)])   # row[2130]\n",
    "X_valid.drop([2130], inplace=True)\n",
    "#display(X_valid[pd.isnull(X_valid.Distance)])   # row[2130]\n",
    "## Now ..ok!!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def. Funct. to Measure Quality of Each Approach to dealing w/missing values\n",
    "We define a function score_dataset() to compare different approaches to dealing with missing values. This function reports the mean absolute error (MAE) from a random forest model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def score_dataset(X_t, X_v, y_t, y_v):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_t, y_t)\n",
    "    y_pred = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 1 (Drop cols with missing values)\n",
    "Since we are working with both training and validation sets, we are careful to drop the same columns in both DataFrames.\n",
    "> JM: dorp missing-values cols BEFORE train_test_split? - future..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop columns with missing values):\n",
      "217286.62174454826\n"
     ]
    }
   ],
   "source": [
    "# X_train.columns, type(X_train.columns)\n",
    "# ['Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n",
    "# 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount'] - pandas.core.indexes.base.Index\n",
    "\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# Drops missing_vals_cols in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "# Error!! - print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
    "# ValueError: Input y contains NaN.\n",
    "\n",
    "## Eliminate NaN in y_train and y_valid\n",
    "# print(y_train[pd.isnull(y_train)])\n",
    "# print(y_valid[pd.isnull(y_valid)])\n",
    "y_train.dropna(inplace=True)\n",
    "y_valid.dropna(inplace=True)\n",
    "# print(y_valid[pd.isnull(y_valid)])\n",
    "\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 2 (Imputation)\n",
    "We'll use *SimpleImputer* to replace missing values with the mean value along each column\n",
    "- filling in the mean value generally performs quiet well, but this varies by dataset.\n",
    "- Statisticians have experimented with more complex ways to determine imputed values (such as regression imputation), but typically give no additional benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Imputation):\n",
      "210048.71943925234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score from Approach 3 (An Extension to Imputation)\n",
    "Next, we impute the missing values, while also keeping track of which values were imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (An Extension to Imputation):\n",
      "205376.4110280374\n"
     ]
    }
   ],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Approach 3 performed slightly worse than Approach 2.    \n",
    "__So, why did imputation perform better than dropping the columns?__    \n",
    "The training data has 10864 rows and 12 columns (JM: df.shape = (2679, 21)), where three (JM: many) columns contain missing data. For each column, less than half of the entries are missing. Thus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2142, 12)\n",
      "Car              16\n",
      "BuildingArea    943\n",
      "YearBuilt       788\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = X_train.isnull().sum()\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As is common, imputing missing values (in Approach 2 and Approach 3) yielded better results, relative to when we simply dropped columns with missing values (in Approach 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUTURE more análysis ... \n",
    "## JM try to fill with mean - similar approach 2\n",
    "- BUT using fillna an col.mean() val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First evaluate (see) NaNs in X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train   - shape: (2142, 12)   - type: <class 'pandas.core.frame.DataFrame'>\n",
      "Car              16\n",
      "BuildingArea    943\n",
      "YearBuilt       788\n",
      "dtype: int64\n",
      "\n",
      "X_valid   - shape: (535, 12)   - type: <class 'pandas.core.frame.DataFrame'>\n",
      "Car              16\n",
      "BuildingArea    943\n",
      "YearBuilt       788\n",
      "dtype: int64\n",
      "\n",
      "y_train   - shape: (2142,)   - type: <class 'pandas.core.series.Series'>\n",
      "Car              16\n",
      "BuildingArea    943\n",
      "YearBuilt       788\n",
      "dtype: int64\n",
      "\n",
      "y_valid   - shape: (535,)   - type: <class 'pandas.core.series.Series'>\n",
      "Car              16\n",
      "BuildingArea    943\n",
      "YearBuilt       788\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_nullsum(pdo_name, pdo):\n",
    "    missings_by_col = X_train.isnull().sum()\n",
    "    print(pdo_name, '  - shape:', pdo.shape, '  - type:', type(pdo))\n",
    "    print(missings_by_col[missings_by_col > 0])\n",
    "    print()\n",
    "\n",
    "d = {'X_train': X_train, 'X_valid': X_valid, 'y_train': y_train, 'y_valid': y_valid }\n",
    "for k,v in d.items():\n",
    "    show_nullsum(k, v)\n",
    "\n",
    "# X_train.isnull().sum()\n",
    "# X_valid.isnull().sum()\n",
    "# y_train.isnull().sum()\n",
    "# y_valid.isnull().sum()\n",
    "# X_train.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train   - shape: (2142, 12)   - type: <class 'pandas.core.frame.DataFrame'>\n",
      "Series([], dtype: int64)\n",
      "\n",
      "y_train   - shape: (2142,)   - type: <class 'pandas.core.series.Series'>\n",
      "Series([], dtype: int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filling all the values with the mean() of corresponding columnn\n",
    "missing_vals_cols = ['Car', 'BuildingArea', 'YearBuilt']\n",
    "\n",
    "for col in missing_vals_cols:\n",
    "    X_train[col] = X_train[col].fillna(X_train[col].mean())\n",
    "    X_valid[col] = X_valid[col].fillna(X_valid[col].mean())\n",
    "\n",
    "# # for pdo in [X_train, y_train]:\n",
    "# #     for col in missing_vals_cols:\n",
    "# #         pdo[col] = pdo[col].fillna(pdo[col].mean())\n",
    "\n",
    "# for col in missing_vals_cols:\n",
    "#     X_valid[col] = X_valid[col].fillna(pdo[col].mean())\n",
    "\n",
    "show_nullsum('X_train', X_train)\n",
    "show_nullsum('y_train', y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Rooms, Distance, Postcode, Bedroom2, Bathroom, Car, Landsize, BuildingArea, YearBuilt, Lattitude, Longtitude, Propertycount]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>3148.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>842.0</td>\n",
       "      <td>150.065789</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>-37.88590</td>\n",
       "      <td>145.09790</td>\n",
       "      <td>3582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>3149.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>-37.88678</td>\n",
       "      <td>145.12748</td>\n",
       "      <td>13366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3055.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>-37.75744</td>\n",
       "      <td>144.94945</td>\n",
       "      <td>7082.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>-37.84150</td>\n",
       "      <td>144.95850</td>\n",
       "      <td>3280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>1965.000000</td>\n",
       "      <td>-37.77800</td>\n",
       "      <td>145.11730</td>\n",
       "      <td>9028.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3124.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>656.0</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>-37.84100</td>\n",
       "      <td>145.08240</td>\n",
       "      <td>8920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>-37.80420</td>\n",
       "      <td>144.98450</td>\n",
       "      <td>4553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>4.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>3188.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>-37.94312</td>\n",
       "      <td>145.02440</td>\n",
       "      <td>2356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>656.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>1972.000000</td>\n",
       "      <td>-37.93271</td>\n",
       "      <td>145.17792</td>\n",
       "      <td>7113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3056.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>150.065789</td>\n",
       "      <td>1961.689349</td>\n",
       "      <td>-37.76430</td>\n",
       "      <td>144.97130</td>\n",
       "      <td>11918.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rooms  Distance  Postcode  Bedroom2  Bathroom  Car  Landsize  \\\n",
       "1509    2.0      13.6    3148.0       2.0       1.0  1.0     842.0   \n",
       "2603    3.0      14.2    3149.0       3.0       2.0  2.0     236.0   \n",
       "2229    3.0       5.2    3055.0       3.0       1.0  2.0     399.0   \n",
       "92      3.0       3.3    3206.0       3.0       2.0  1.0     177.0   \n",
       "1773    4.0      13.9    3108.0       4.0       2.0  2.0     716.0   \n",
       "...     ...       ...       ...       ...       ...  ...       ...   \n",
       "1225    3.0       7.8    3124.0       3.0       2.0  2.0     656.0   \n",
       "1748    3.0       1.6    3066.0       2.0       2.0  2.0       0.0   \n",
       "2297    4.0      13.8    3188.0       4.0       1.0  1.0     700.0   \n",
       "2342    3.0      18.8    3170.0       3.0       1.0  2.0     656.0   \n",
       "1023    3.0       5.2    3056.0       3.0       2.0  1.0     316.0   \n",
       "\n",
       "      BuildingArea    YearBuilt  Lattitude  Longtitude  Propertycount  \n",
       "1509    150.065789  1972.000000  -37.88590   145.09790         3582.0  \n",
       "2603    138.000000  2006.000000  -37.88678   145.12748        13366.0  \n",
       "2229    105.000000  1950.000000  -37.75744   144.94945         7082.0  \n",
       "92      181.000000  1880.000000  -37.84150   144.95850         3280.0  \n",
       "1773    155.000000  1965.000000  -37.77800   145.11730         9028.0  \n",
       "...            ...          ...        ...         ...            ...  \n",
       "1225    225.000000  2009.000000  -37.84100   145.08240         8920.0  \n",
       "1748    159.000000  1920.000000  -37.80420   144.98450         4553.0  \n",
       "2297    175.000000  1950.000000  -37.94312   145.02440         2356.0  \n",
       "2342    107.000000  1972.000000  -37.93271   145.17792         7113.0  \n",
       "1023    150.065789  1961.689349  -37.76430   144.97130        11918.0  \n",
       "\n",
       "[535 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_valid[pd.isnull(X_valid['Car'])])\n",
    "display(X_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
