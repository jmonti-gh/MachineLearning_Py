{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle - Learn: Intermediate Machine Learning\n",
    "- https://www.kaggle.com/learn/intermediate-machine-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Pipelines\n",
    "- A critical skill for deploying (and even testing) complex models with pre-processing.\n",
    "- A critical skill for deploying (and even testing) complex models with pre-processing.\n",
    "\n",
    "### Intro\n",
    "- There's a lot of non-numeric data out there. Here's how to use it for machine learning.\n",
    "- What is a Catagorical Variable?\n",
    "    - Options like: Never, Rarely, Most days, Every day.\n",
    "    - Brands Names, like: Honda, Toyota, Ford\n",
    "- You will get an error if you try to plug these variables into most machine learning models in Python without preprocessing them first.\n",
    "- Three approaches for handling this type of data:\n",
    "    1. Drop Categorical Variables: if cat-cols don't contain useful info.\n",
    "        - this only work if cols didn't contain useful info.\n",
    "    2. Ordinal Encoding: assigns each unique value to a different integer.\n",
    "        - Indisputable ranking to the categories. Ordinal Variables.\n",
    "        - ex. Never(0), Rarely(1), Most days(2), Every day(3).\n",
    "    3. One-Hot Encoding: new columns related with categories.\n",
    "        - No clear ordering of the categories. Nominal Variables.\n",
    "        - ex. diff. Colors, diff Cars Brands  - w/o intrinsic ranking of vars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Case\n",
    "- first load and separate of training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2677 entries, 0 to 2678\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Suburb         2677 non-null   object \n",
      " 1   Address        2677 non-null   object \n",
      " 2   Rooms          2677 non-null   float64\n",
      " 3   Type           2677 non-null   object \n",
      " 4   Price          2677 non-null   float64\n",
      " 5   Method         2677 non-null   object \n",
      " 6   SellerG        2677 non-null   object \n",
      " 7   Date           2677 non-null   object \n",
      " 8   Distance       2677 non-null   float64\n",
      " 9   Postcode       2677 non-null   float64\n",
      " 10  Bedroom2       2677 non-null   float64\n",
      " 11  Bathroom       2677 non-null   float64\n",
      " 12  Car            2655 non-null   float64\n",
      " 13  Landsize       2677 non-null   float64\n",
      " 14  BuildingArea   1503 non-null   float64\n",
      " 15  YearBuilt      1692 non-null   float64\n",
      " 16  CouncilArea    2129 non-null   object \n",
      " 17  Lattitude      2677 non-null   float64\n",
      " 18  Longtitude     2677 non-null   float64\n",
      " 19  Regionname     2677 non-null   object \n",
      " 20  Propertycount  2677 non-null   float64\n",
      "dtypes: float64(13), object(8)\n",
      "memory usage: 460.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('files/min_melb_data.csv')\n",
    "\n",
    "## JM: to see (and change) rows w/ NaN values\n",
    "#df.info()       # Rooms has one?\n",
    "df[df['Rooms'].isnull()]        # index 1920 all NaNs except Suburb & Addr\n",
    "# future: find the exactly case of 2 not NaN & the other non-NaN\n",
    "\n",
    "# Now delete this row\n",
    "df.drop([1920], inplace=True)\n",
    "#df[df['Rooms'].isnull()] \n",
    "# df.info()     # -> there are still a lot of cols with one missing\n",
    "df[df['Date'].isnull()]         # index 2130 \n",
    "df.drop([2130], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2141, 20)\n",
      "(2141, 16)\n",
      "['Type', 'Method', 'Regionname']\n",
      "['Type', 'Method', 'Regionname', 'Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude', 'Propertycount']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>h</td>\n",
       "      <td>VB</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>-37.9270</td>\n",
       "      <td>145.0267</td>\n",
       "      <td>6938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>u</td>\n",
       "      <td>PI</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3163.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.8982</td>\n",
       "      <td>145.0625</td>\n",
       "      <td>7822.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "817     h     VB  Southern Metropolitan    3.0      10.7    3187.0       3.0   \n",
       "1366    u     PI  Southern Metropolitan    1.0      11.4    3163.0       1.0   \n",
       "\n",
       "      Bathroom  Landsize  Lattitude  Longtitude  Propertycount  \n",
       "817        2.0     619.0   -37.9270    145.0267         6938.0  \n",
       "1366       1.0       0.0   -37.8982    145.0625         7822.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Continue w/\n",
    "#   - separate target and predictors\n",
    "#   - separate training and validation datasets\n",
    "#   - handel missing values\n",
    "#   - select categorical cols w low cardinality (nunique < 10)\n",
    "#   - select numerical cols\n",
    "\n",
    "# basic target and predictors\n",
    "y = df.Price\n",
    "X = df.drop(['Price'], axis=1)\n",
    "\n",
    "# split train and validation datasets\n",
    "X_train_f, X_valid_f, y_train, y_valid = train_test_split(X, y, random_state=0,\n",
    "                                                      train_size=0.8,\n",
    "                                                      test_size=0.2)\n",
    "# X_train_f.info()\n",
    "print(X_train_f.shape)\n",
    "\n",
    "# Missing Values Handling: drops cols w/missing values (simplest approach)\n",
    "cols_w_missing = [col for col in X_train_f.columns\n",
    "                   if X_train_f[col].isnull().any()]     # cols_w_missing list\n",
    "X_train_f.drop(cols_w_missing, axis=1, inplace=True)\n",
    "X_valid_f.drop(cols_w_missing, axis=1, inplace=True)\n",
    "\n",
    "print(X_train_f.shape)      # ok! only loose 4 columns for NaNs\n",
    "\n",
    "## \"Cardinality\" means the number of unique values in a column\n",
    "# Mk a list of categorical columns with relatively low cardinality (convenient\n",
    "# but arbitrary)    - cnmae ??, better low_cname -> lcn\n",
    "low_cardinality_cols = [cname for cname in X_train_f.columns\n",
    "                        if X_train_f[cname].nunique() < 10 and\n",
    "                        X_train_f[cname].dtype == 'object']\n",
    "\n",
    "print(low_cardinality_cols)                     \n",
    "\n",
    "# Select numerical columns  - cname ??. better col\n",
    "numerical_cols = [cname for cname in X_train_f.columns\n",
    "                  if X_train_f[cname].dtype in ['int64', 'float64']]\n",
    "# ..dtype in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']\n",
    "# or ..dtype != 'object'\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "print(my_cols)\n",
    "X_train = X_train_f[my_cols].copy()\n",
    "X_valid = X_valid_f[my_cols].copy()\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we obtain a list of all of the categorical variables in the training data.\n",
    "\n",
    "We do this by checking the data type (or dtype) of each column. The object dtype indicates a column has text (there are other things it could theoretically be, but that's unimportant for our purposes). For this dataset, the columns with text indicate categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Method', 'Regionname']\n",
      "['Type', 'Method', 'Regionname']\n",
      "Categorical variables: ['Type', 'Method', 'Regionname']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Method', 'Regionname', 'Rooms', 'Distance', 'Postcode',\n",
       "       'Bedroom2', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude',\n",
       "       'Propertycount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = X_train.dtypes == 'object'  # bool serie, True for objects cols\n",
    "#s[s].index, type(s[s].index)\n",
    "object_cols = list(s[s].index)\n",
    "# s.index\n",
    "print(object_cols)\n",
    "\n",
    "# JM other way\n",
    "o_c = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "print(o_c)\n",
    "\n",
    "print('Categorical variables:', object_cols)\n",
    "X_train.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to Measure Quality of Each Approach\n",
    "We define a function score_dataset() to compare the three different approaches to dealing with categorical variables. This function reports the mean absolute error (MAE) from a random forest model. In general, we want the MAE to be as low as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score from Approach 1 (Drop Categorical Variables)\n",
    "We drop the object columns with the select_dtypes() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "209800.12821606253\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score from Approach 2 (Ordinal Encoding)\n",
    "Scikit-learn has a OrdinalEncoder class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "201420.60514303483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "#label_X_train.columns\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above, for each column, we randomly assign each unique value to a different integer. This is a common approach that is simpler than providing custom labels; however, we can expect an additional boost in performance if we provide better-informed labels for all ordinal variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score from Approach 3 (One-Hot Encoding)\n",
    "We use the OneHotEncoder class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior.\n",
    "- We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data, and\n",
    "- setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).    \n",
    "\n",
    "To use the encoder, we supply only the categorical columns that we want to be one-hot encoded. For instance, to encode the training data, we supply X_train[object_cols]. (object_cols in the code cell below is a list of the column names with categorical data, and so X_train[object_cols] contains all of the categorical data in the training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "198747.04837686566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which approach is best?Â¶\n",
    "\n",
    "In this case, dropping the categorical columns (Approach 1) performed worst, since it had the highest MAE score. As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other.\n",
    "\n",
    "In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis.\n",
    "## Conclusion\n",
    "\n",
    "The world is filled with categorical data. You will be a much more effective data scientist if you know how to use this common data type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
