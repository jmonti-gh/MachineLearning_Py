{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle - Learn: Intermediate Machine Learning\n",
    "- https://www.kaggle.com/learn/intermediate-machine-learning\n",
    "> JM-Future: mk a program to obtain a mini dataset from a big dataset\n",
    "mantaining cols names and a prportional number of NaN in every col?, ex. to conver melb_data to min_melb_data, three versions or put the % of shrink"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Pipelines\n",
    "- A critical skill for deploying (and even testing) complex models with pre-processing.\n",
    "- Wow to use pipelines to clean up your modeling code.\n",
    "\n",
    "### Intro\n",
    "-  pipeline bundles (agrupa) preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n",
    "- __Cleaner Code:__ you won't need to manually keep track of your training and validation data at each step.\n",
    "- __Fewer Bugs:__ There are fewer opportunities to misapply a step or forget a preprocessing step.\n",
    "- __Easier to Productionize:__ help to transition a model from a prototype to something deployable at scale. \n",
    "- __More Options for Model Validation:__ future example with cross_validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Case\n",
    "- at first, the steps to have training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3104.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>-37.78460</td>\n",
       "      <td>145.09350</td>\n",
       "      <td>7809.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Eastern Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>3081.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>-37.74350</td>\n",
       "      <td>145.04860</td>\n",
       "      <td>2947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>2</td>\n",
       "      <td>11.2</td>\n",
       "      <td>3145.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.86720</td>\n",
       "      <td>145.04320</td>\n",
       "      <td>8801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13170</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>3</td>\n",
       "      <td>19.6</td>\n",
       "      <td>3076.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-37.63854</td>\n",
       "      <td>145.05179</td>\n",
       "      <td>10926.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Southern Metropolitan</td>\n",
       "      <td>4</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3163.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>-37.89310</td>\n",
       "      <td>145.04790</td>\n",
       "      <td>7822.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method             Regionname  Rooms  Distance  Postcode  Bedroom2  \\\n",
       "664      h      S  Southern Metropolitan      3       9.2    3104.0       3.0   \n",
       "3270     h      S   Eastern Metropolitan      2      10.5    3081.0       2.0   \n",
       "3873     h      S  Southern Metropolitan      2      11.2    3145.0       2.0   \n",
       "13170    h      S  Northern Metropolitan      3      19.6    3076.0       3.0   \n",
       "1730     h      S  Southern Metropolitan      4      11.4    3163.0       3.0   \n",
       "\n",
       "       Bathroom  Car  Landsize  BuildingArea  YearBuilt  Lattitude  \\\n",
       "664         2.0  2.0     368.0         177.0     2009.0  -37.78460   \n",
       "3270        1.0  2.0     586.0          80.0     1955.0  -37.74350   \n",
       "3873        1.0  1.0     348.0           NaN        NaN  -37.86720   \n",
       "13170       1.0  1.0     521.0           NaN        NaN  -37.63854   \n",
       "1730        2.0  2.0     687.0         237.0     1983.0  -37.89310   \n",
       "\n",
       "       Longtitude  Propertycount  \n",
       "664     145.09350         7809.0  \n",
       "3270    145.04860         2947.0  \n",
       "3873    145.04320         8801.0  \n",
       "13170   145.05179        10926.0  \n",
       "1730    145.04790         7822.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Type             False\n",
       "Method           False\n",
       "Regionname       False\n",
       "Rooms            False\n",
       "Distance         False\n",
       "Postcode         False\n",
       "Bedroom2         False\n",
       "Bathroom         False\n",
       "Car               True\n",
       "Landsize         False\n",
       "BuildingArea      True\n",
       "YearBuilt         True\n",
       "Lattitude        False\n",
       "Longtitude       False\n",
       "Propertycount    False\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10185, 15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    10138\n",
      "True        47\n",
      "Name: Car, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('files/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = df.Price\n",
    "X = df.drop(['Price'], axis=1)\n",
    "\n",
    "# #JM: Other ways to select predictors\n",
    "# leftcols = [col for col in df.columns if col != 'Price']\n",
    "# df1 = df.loc[:, leftcols]\n",
    "# df2 = df[leftcols]\n",
    "# print(df.shape, df1.shape, df2.shape)\n",
    "# display(df.head(2), df1.head(2), df2.head(2))\n",
    "\n",
    "# Divide data into trining and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [col for col in X_train_full.columns if\n",
    "                    X_train_full[col].nunique() < 10 and df[col].dtype == 'object']\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [col for col in X_train_full.columns if\n",
    "                  X_train_full[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "# #JM: Comparing collections or dfs equality\n",
    "# num_cols = [col for col in X_train_full.columns if \n",
    "#             X_train_full[col].dtype != 'object']\n",
    "# #print(numerical_cols == num_cols, numerical_cols is num_cols)   # True False\n",
    "# df2 = df.copy()\n",
    "# print(df2 == df, df2 is df)   # True..True..True.. False\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "display(X_train.head(), X_train.isnull().any(), X_train.shape)\n",
    "print(X_train.Car.isnull().value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the data contains both categorical data and columns with missing values. With a pipeline, it's easy to deal with both!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the full pipeline in three steps\n",
    "1. Define Preprocessing Steps. *ColumnTransformer* class\n",
    "2. Define the Model. *RandomForest* by ex.\n",
    "3. Create and Evaluate the Pipeline. *Pipeline* class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Preprocessing Steps.\n",
    "- imputes missing values in numerical data, and\n",
    "- imputes missing values and applies a one-hot encoding to categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## JM trys\n",
    "# m1 = RandomForestRegressor()\n",
    "# m1.get_params()                 # n_estimators defualt = 100\n",
    "#                                 # random_state = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create and Evaluate the Pepeline\n",
    "*Pipeline* class to define a pipeline that bundles the preprocessing and modeling steps. Things to notice:\n",
    "- With the pipeline, we preprocess the training data and fit the model in a single line of code.\n",
    "- With the pipeline, we supply the unprocessed features in X_valid to the predict() method, and the pipeline automatically preprocesses the features before generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 163987.3804899362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)])\n",
    "\n",
    "# Preprocessing of training data, fit model\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Pipelines are valuable for cleaning up machine learning code and avoiding errors, and are especially useful for workflows with sophisticated data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
