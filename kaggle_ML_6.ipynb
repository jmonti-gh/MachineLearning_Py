{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle - Learn: Intro to Machine Learning\n",
    "- https://www.kaggle.com/learn/intro-to-machine-learning\n",
    "## 6. Random Forests\n",
    "- Using a more sophisticated machine learning algorithm.\n",
    "(Even today's most sophisticated modeling techniques face this tension between underfitting and overfitting)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Model \n",
    "- uses many trees, and it makes a prediction by averaging the predictions of each component tree.\n",
    "-  It generally has much better predictive accuracy than a single decision tree and it works well with default parameters\n",
    "> If you keep modeling, you can learn more models with even better performance, *but many of those are sensitive to getting the right parameters*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "At the begin of the analysis we load de dataset an at the end we have:\n",
    "- train_X (training features), val_X (validation_features)\n",
    "- train_y (training target), val_y (validation target - also known as y_true as opposite to y_pred)\n",
    "    - You use train_X and train_y (the training data) to .fit the model.\n",
    "    - You use val_X to .predict unknown target.\n",
    "    - You use val_y to calculate MAE (val_y, val_pred) or (y_true, y_pred)\n",
    "Let's make the load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('min_melb_data.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "y = df.Price\n",
    "X = df[['Rooms', 'Bathroom', 'Landsize', 'BuildingArea','YearBuilt', 'Lattitude', 'Longtitude']]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# display(train_X)\n",
    "# print(train_y)\n",
    "# display(val_X)\n",
    "# print(val_y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now... we build a random forest model similarly to how we built a decision tree in scikit-learn - this time using the RandomForestRegressor class instead of DecisionTreeRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 214,229.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "\n",
    "val_pred = forest_model.predict(val_X)\n",
    "mae = mean_absolute_error(val_y, val_pred)\n",
    "\n",
    "print(f'MAE: {mae:,.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More\n",
    "- There is likely room for further improvement, but this is a big improvement over the best decision tree error we saw previously.\n",
    "- There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Random Forests\n",
    "### 0 - import libraries + make load and prepare train and val features an targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All commented cause they are previously imported\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.emsemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "home_data = pd.read_csv('train.csv')\n",
    "\n",
    "y = home_data['SalePrice']      # using direct location instead of dot-notation loc.\n",
    "X = home_data[['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- make random_forest model, calculate forest_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using RandomForest model: 23,009.21\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "forest_y = forest_model.predict(val_X)\n",
    "forest_mae = mean_absolute_error(val_y, forest_y)\n",
    "print(f'MAE using RandomForest model: {forest_mae:,.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.- make decision_tree model (def params), calculate dtree_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE using DecisionTree model: 32,966.45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtree_model = DecisionTreeRegressor(random_state=1)\n",
    "dtree_model.fit(train_X, train_y)\n",
    "dtree_y = dtree_model.predict(val_X)\n",
    "dtree_mae = mean_absolute_error(val_y, dtree_y)\n",
    "print(f'MAE using DecisionTree model: {dtree_mae:,.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- make decision_tree model (max_leaf_nodes param for best case), calculate dbtree_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min MAE: 27,203.78  <-- max_leaf_nodes = 82\n"
     ]
    }
   ],
   "source": [
    "# function to get mae with max_leaf_nodes value as a parameter:\n",
    "def get_mae (t_X, v_X, t_y, v_y, mlnodes):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=mlnodes, random_state=0)\n",
    "    model.fit(t_X, t_y)\n",
    "    y_pred = model.predict(v_X)\n",
    "    mae = mean_absolute_error(v_y, y_pred)\n",
    "    return(mae)\n",
    "\n",
    "resdic = dict()        # results dictionary {mlnodes1: mae1, mlnodes2: mae2, ..., mlnodesN: maeN}\n",
    "\n",
    "for mlnds in range(2, 400):\n",
    "    mae = get_mae(train_X, val_X, train_y, val_y, mlnds)\n",
    "    resdic[mlnds] = mae\n",
    "\n",
    "dbtree_mae = min(resdic.values())\n",
    "for it in resdic.keys():\n",
    "    if resdic[it] == dbtree_mae:\n",
    "        mlnodes_dbtree = it\n",
    "        break\n",
    "\n",
    "print(f'min MAE: {dbtree_mae:,.2f}  <-- max_leaf_nodes = {mlnodes_dbtree}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Validation (via MAE) COMPARATIVE\n",
      "--------------------------------------\n",
      "RandomForestRegressor __default: 23,009.21\n",
      "DecisionTreeRegressor __default: 32,966.45\n",
      "DecisionTreeRegressor __best: 27,203.78  <-- max_leaf_nodes = 82\n"
     ]
    }
   ],
   "source": [
    "print('Model Validation (via MAE) COMPARATIVE')\n",
    "print('--------------------------------------')\n",
    "print(f'RandomForestRegressor __default: {forest_mae:,.2f}')\n",
    "print(f'DecisionTreeRegressor __default: {dtree_mae:,.2f}')\n",
    "print(f'DecisionTreeRegressor __best: {dbtree_mae:,.2f}  <-- max_leaf_nodes = {mlnodes_dbtree}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
